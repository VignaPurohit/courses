{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxduQznAp2Ax"
      },
      "source": [
        "### Overview\n",
        "\n",
        "Zonal Statistics is used to summarises the values of a raster dataset within the zones of a vector dataset. Here we select all Admin1 units of a country and calculate a sum of nighttime light pixel intensities over multiple years. This is a large computation that is enabled by cloud-hosted NightTime Lights data in the Cloud-Optimized GeoTIFF (COG) format and parallel computing on a local dask cluster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE4NnpBGwvVm"
      },
      "source": [
        "### Setup and Data Download\n",
        "\n",
        "The following blocks of code will install the required packages and download the datasets to your Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXQvU68ohMWA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !apt install libspatialindex-dev\n",
        "    !pip install fiona shapely pyproj rtree\n",
        "    !pip install geopandas\n",
        "    !pip install rioxarray\n",
        "    !pip install regionmask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8meeR2XidDA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import rioxarray as rxr\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import dask\n",
        "import regionmask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G8scYV4vb0z"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import rasterio\n",
        "warnings.filterwarnings(\"ignore\", category=rasterio.RasterioDeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3a5EQJmWhtj"
      },
      "outputs": [],
      "source": [
        "from dask.distributed import Client, progress\n",
        "client = Client()  # set up local cluster on the machine\n",
        "client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx-tBAfE6aUU"
      },
      "outputs": [],
      "source": [
        "dask.config.set({\"array.slicing.split_large_chunks\": False})\n",
        "dask.config.set({'array.chunk-size': '32MiB'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTO2zAqXidzr"
      },
      "outputs": [],
      "source": [
        "data_folder = 'data'\n",
        "output_folder = 'output'\n",
        "\n",
        "if not os.path.exists(data_folder):\n",
        "    os.mkdir(data_folder)\n",
        "if not os.path.exists(output_folder):\n",
        "    os.mkdir(output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-cvhSW4igBo"
      },
      "outputs": [],
      "source": [
        "def download(url):\n",
        "    filename = os.path.join(data_folder, os.path.basename(url))\n",
        "    if not os.path.exists(filename):\n",
        "        from urllib.request import urlretrieve\n",
        "        local, _ = urlretrieve(url, filename)\n",
        "        print('Downloaded ' + local)\n",
        "\n",
        "admin1_zipfile = 'ne_10m_admin_1_states_provinces.zip'\n",
        "admin1_url = 'https://naciscdn.org/naturalearth/10m/cultural/'\n",
        "\n",
        "download(admin1_url + admin1_zipfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6kEXzadw4k7"
      },
      "source": [
        "### Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKGBKNspw7Pp"
      },
      "source": [
        "First we will read the GDL shapefile and filter to a country.\n",
        "The 'adm1_code' column contains a unique id for all the counties present in the state, but it is of `object` type. We need to convert it to `int` type to be used in xarray.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leakJcQWjrJ4"
      },
      "outputs": [],
      "source": [
        "country_code = 'LK'\n",
        "admin1_file_path = os.path.join(data_folder, admin1_zipfile)\n",
        "\n",
        "admin1_df = gpd.read_file(admin1_file_path)\n",
        "\n",
        "zones = admin1_df[admin1_df['iso_a2'] == country_code][['adm1_code', 'name', 'iso_a2', 'geometry']].copy()\n",
        "zones['id'] = zones.reset_index().index + 1\n",
        "zones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K81OSYEovx7C"
      },
      "outputs": [],
      "source": [
        "bbox = zones.total_bounds\n",
        "bbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUPCWUQByt0C"
      },
      "source": [
        "Next we read the NTL files and create an XArray object.\n",
        "\n",
        "These files were download from [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YGIVCD) and converted to Cloud-Optimized GeoTIFFs using GDAL.\n",
        "\n",
        " Example command\n",
        "\n",
        " ```\n",
        " gdalwarp -of COG 2021_HasMask/LongNTL_2021.tif 2021.tif \\\n",
        "  -te -180 -90 180 90 -dstnodata 0 \\\n",
        "  -co COMPRESS=DEFLATE -co PREDICTOR=2 -co NUM_THREADS=ALL_CPUS\n",
        "```\n",
        "\n",
        "The resulting files are now hosted on a Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSsv5cCJ08yn"
      },
      "outputs": [],
      "source": [
        "start_year = 2015\n",
        "end_year = 2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5xBMeBnYbwR"
      },
      "outputs": [],
      "source": [
        "\n",
        "ntl_folder = 'https://storage.googleapis.com/spatialthoughts-public-data/ntl/npp_viirs_ntl'\n",
        "\n",
        "da_list = []\n",
        "\n",
        "for year in range(start_year, end_year + 1):\n",
        "  cog_url = f'{ntl_folder}/{year}.tif'\n",
        "  da = rxr.open_rasterio(\n",
        "      cog_url,\n",
        "      chunks=True).rio.clip_box(*bbox)\n",
        "  dt = pd.to_datetime(year, format='%Y')\n",
        "  da = da.assign_coords(time = dt)\n",
        "  da = da.expand_dims(dim=\"time\")\n",
        "  da_list.append(da)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgzALGzsDKyn"
      },
      "outputs": [],
      "source": [
        "ntl_datacube = xr.concat(da_list, dim='time').chunk('auto')\n",
        "ntl_datacube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jdDErnrGZav"
      },
      "outputs": [],
      "source": [
        "ntl_datacube = ntl_datacube.sel(band=1, drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uND6MOddOoyJ"
      },
      "source": [
        "### Zonal Stats\n",
        "\n",
        "Now we will extract the sum of the raster pixel values for every admin1 region in the selected countrey."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL1frS4hnnY9"
      },
      "source": [
        "First, we need to convert the GeoDataFrame to a XArray Dataset. We will be using the `regionmask` module for that. It takes geodataframe, it's unique value as integer and converts the geodataframe into  a `xarray dataset` having dimension and coordinates same as of given input xarray dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d91yeyUeNO_P"
      },
      "outputs": [],
      "source": [
        "# Create mask of multiple regions from shapefile\n",
        "mask = regionmask.mask_3D_geopandas(\n",
        "        zones,\n",
        "        ntl_datacube.x,\n",
        "        ntl_datacube.y,\n",
        "        drop=True,\n",
        "        numbers=\"id\",\n",
        "        overlap=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6kJPigONnnm"
      },
      "outputs": [],
      "source": [
        "ntl_datacube = ntl_datacube.where(mask).chunk('auto')\n",
        "ntl_datacube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS2JTRlcNyAW"
      },
      "outputs": [],
      "source": [
        "grouped = ntl_datacube.groupby('time').sum(['x','y'])\n",
        "grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U7v2X2YPz35"
      },
      "outputs": [],
      "source": [
        "%time grouped = grouped.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-14twvJqN7ri"
      },
      "outputs": [],
      "source": [
        "stats = grouped.drop('spatial_ref').to_dataframe('sum').reset_index()\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCobZY6zAFbM"
      },
      "outputs": [],
      "source": [
        "stats['year'] = stats.time.dt.year\n",
        "stats = stats.rename(columns={'region': 'id'})\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy9QGTyFVb7f"
      },
      "outputs": [],
      "source": [
        "results = zones.merge(stats, on='id')\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHCddLI2Sp9o"
      },
      "source": [
        "Finally, we save the result to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxQiAR26BJ6G"
      },
      "outputs": [],
      "source": [
        "output = results[['adm1_code', 'name', 'iso_a2', 'year', 'sum']]\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdAyegoO61Er"
      },
      "outputs": [],
      "source": [
        "output_file = 'output.csv'\n",
        "output_path = os.path.join(output_folder, output_file)\n",
        "\n",
        "output.to_csv(output_path, index=False)\n",
        "print('Successfully written output file at {}'.format(output_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "\n",
        "Change the code to calculate the Zonal Statistics for all admin1 units in your chosen country."
      ],
      "metadata": {
        "id": "sbkK6btkQjlM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}