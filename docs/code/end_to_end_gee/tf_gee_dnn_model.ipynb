{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdlQT13vc_4d"
      },
      "source": [
        "## Deep Neural Network Classification with Google Earth Engine\n",
        "\n",
        "Adapted from https://developers.google.com/earth-engine/guides/tf_examples#multi-class-prediction-with-a-dnn\n",
        "\n",
        "This notebook shows how to build, train and use a DNN model for multi-class classification on satellite imagery using Tensorflow and Keras.\n",
        "\n",
        "The training points and Sentinel-2 composite are created in Google Earth Engine. The region in the Arkavathy River Basin in South India. We aim to carry out a landcover in 4 classes: Urban, Bare, Water and Vegetation using an offline training and prediction workflow.\n",
        "\n",
        "![Classification Results](https://courses.spatialthoughts.com/images/end_to_end_gee/tf_classification.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdCbLJgjEj49"
      },
      "source": [
        "#### Installation and Imports\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtFBgy95EJ-l"
      },
      "source": [
        "#### Configuration\n",
        "\n",
        "The following cells define the configuration options and GCS paths. You will have to change them to appropriate values for your own account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_8RNV3gqVbca"
      },
      "outputs": [],
      "source": [
        "PROJECT = 'deep-learning-287813'\n",
        "REGION = 'us-central1'\n",
        "\n",
        "FEATURE_NAMES = [ 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'landcover']\n",
        "BANDS = [ 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']\n",
        "\n",
        "LABEL = 'landcover'\n",
        "N_CLASSES = 4\n",
        "TRAIN_FILE_PATH = 'gs://earthengine-tf/arkavathy_training.tfrecord.gz'\n",
        "TEST_FILE_PATH = 'gs://earthengine-tf/arkavathy_testing.tfrecord.gz'\n",
        "MODEL_DIR = 'gs://earthengine-tf/arkavathy_model'\n",
        "MODEL_NAME = 'arkavathy_tf_model'\n",
        "VERSION_NAME = 'v1'\n",
        "EXPORTED_IMAGE_PREFIX = 'arkavathy_image'\n",
        "OUTPUT_IMAGE_FILE = 'gs://earthengine-tf/arkavathy_tf_classified.TFRecord'\n",
        "OUTPUT_ASSET_ID = 'users/ujavalgandhi/tf/arkavathy_classified_dnn'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1JfkSrHxSzgt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import ee\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGcy9q1BGNaS"
      },
      "source": [
        "#### Initialization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBDpejXVFIee"
      },
      "source": [
        "Authnenticate and initialize GEE API. This needs to done once per colab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vOGlQ3bEYSwI"
      },
      "outputs": [],
      "source": [
        "cloud_project = PROJECT\n",
        "\n",
        "try:\n",
        "    ee.Initialize(project=cloud_project)\n",
        "except:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project=cloud_project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik7JHvyWN65w"
      },
      "source": [
        "## Data Pre-Processing\n",
        "\n",
        "The training data for our model is in the TFRecord format and stored on GCS. As we are using Keras API, we need to parse the data and convert it into tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j9IuAMLTVX8"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from the TFRecord file in Cloud Storage.\n",
        "train_dataset = tf.data.TFRecordDataset(TRAIN_FILE_PATH, compression_type='GZIP')\n",
        "# Print the first record to check.\n",
        "iter(train_dataset).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vJ4U8BdRU4Pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b55fbf-23b7-4d3a-c1bf-1502a9bb2ccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'B1': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.1874], dtype=float32)>,\n",
              "  'B11': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5219], dtype=float32)>,\n",
              "  'B12': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5526], dtype=float32)>,\n",
              "  'B2': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.305], dtype=float32)>,\n",
              "  'B3': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.3372], dtype=float32)>,\n",
              "  'B4': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.3664], dtype=float32)>,\n",
              "  'B5': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.3921], dtype=float32)>,\n",
              "  'B6': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.3798], dtype=float32)>,\n",
              "  'B7': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.3701], dtype=float32)>,\n",
              "  'B8': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.3422], dtype=float32)>,\n",
              "  'B8A': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.3539], dtype=float32)>,\n",
              "  'B9': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.325], dtype=float32)>},\n",
              " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# List of fixed-length features, all of which are float32.\n",
        "columns = [\n",
        "  tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in FEATURE_NAMES\n",
        "]\n",
        "\n",
        "# Dictionary with names as keys, features as values.\n",
        "features_dict = dict(zip(FEATURE_NAMES, columns))\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "\n",
        "  Read a serialized example into the structure defined by featuresDict.\n",
        "\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of the predictors dictionary and the label, cast to an `int32`.\n",
        "  \"\"\"\n",
        "  parsed_features = tf.io.parse_single_example(example_proto, features_dict)\n",
        "  labels = parsed_features.pop(LABEL)\n",
        "  return parsed_features, tf.cast(labels, tf.int32)\n",
        "\n",
        "# Map the function over the dataset.\n",
        "parsed_dataset = train_dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "\n",
        "# Print the first parsed record to check.\n",
        "iter(parsed_dataset).next()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MiX_T701VW7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bcac7d-dc6c-4495-fe3c-590b5cdac9bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
              " array([[0.1874, 0.5219, 0.5526, 0.305 , 0.3372, 0.3664, 0.3921, 0.3798,\n",
              "         0.3701, 0.3422, 0.3539, 0.325 ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 0., 0., 0.]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Keras requires inputs as a tuple.  Note that the inputs must be in the\n",
        "# right shape.  Also note that to use the categorical_crossentropy loss,\n",
        "# the label needs to be turned into a one-hot vector.\n",
        "def to_tuple(inputs, label):\n",
        "  return (tf.transpose(list(inputs.values())),\n",
        "          tf.one_hot(indices=label, depth=N_CLASSES))\n",
        "\n",
        "# Map the to_tuple function\n",
        "input_dataset = parsed_dataset.map(to_tuple)\n",
        "iter(input_dataset).next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yATQihOMTLJg"
      },
      "source": [
        "The dataset is now in the correct shape to be used with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ6BBi5lS7S-"
      },
      "source": [
        "## Building and Training the DNN\n",
        "\n",
        "We will use a simple DNN architecture with 1 input layer, 1 hidden-layer and 1 output layer.\n",
        "\n",
        "\n",
        "* **Input Layer**: The training data contains 12 input band values, so the input will be an array of 12 values.\n",
        "* **Hidden Layer**: 64-nodes with ReLU activation function and 0.2 dropout rate.\n",
        "* **Output Layer**:  4 nodes with softmax activation to predict the class probability of each input.\n",
        "\n",
        "> The dropout layer will set the output of 20% of the nodes from the hidden layer to 0 for each epoch. Dropout layers help in avoiding overfitting.\n",
        "\n",
        "![Architecture](https://courses.spatialthoughts.com/images/end_to_end_gee/dnn_architecture.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62qEwqzSNgMU"
      },
      "outputs": [],
      "source": [
        "# Define the layers in the model.\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(N_CLASSES, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# Compile the model with the specified loss function.\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Shuffle the training and data and create a batch\n",
        "# 8 training samples per batch\n",
        "input_dataset = input_dataset.shuffle(64).batch(8)\n",
        "\n",
        "# Fit the model to the training data.\n",
        "model.fit(x=input_dataset, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4v5keB8IPXjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "84876f2e-a075-422e-a417-7e0e82844f64"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │             \u001b[38;5;34m832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                │             \u001b[38;5;34m260\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,278\u001b[0m (12.81 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,278</span> (12.81 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,092\u001b[0m (4.27 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,092</span> (4.27 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,186\u001b[0m (8.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,186</span> (8.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXykm-R8c9VH"
      },
      "source": [
        "Check model accuracy on the validation fraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U8UrJLp6WKPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f62661-31ec-495b-d5de-89f11f74784c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8928 - loss: 0.3761\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2726761996746063, 0.9269663095474243]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "test_dataset = (\n",
        "  tf.data.TFRecordDataset(TEST_FILE_PATH, compression_type='GZIP')\n",
        "    .map(parse_tfrecord, num_parallel_calls=5)\n",
        "    .map(to_tuple)\n",
        "    .batch(8))\n",
        "\n",
        "model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6s8AiuxUklj"
      },
      "outputs": [],
      "source": [
        "model.save(MODEL_DIR, save_format='tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iInWFeCkalyy"
      },
      "source": [
        "## Classifying the Image\n",
        "\n",
        "We will now use the DNN model to predict the output class for all pixels of the input image. The input composite for the entire basin has been exported to GCS. We will read the image and run prediction for each image path.\n",
        "\n",
        "When you export an image from Earth Engine as TFRecords, you get 2 files\n",
        "\n",
        "\n",
        "*   `.tfrecord.gz` files containing image patches\n",
        "*   `mixer.json` file containing image metadata and georeferencing information\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bVHiG89PYwtf"
      },
      "outputs": [],
      "source": [
        "# Get a list of all the files in the output bucket.\n",
        "files_list = !gsutil ls 'gs://earthengine-tf'\n",
        "\n",
        "# Get only the files generated by the image export.\n",
        "exported_files_list = [s for s in files_list if EXPORTED_IMAGE_PREFIX in s]\n",
        "\n",
        "# Get the list of image files and the JSON mixer file.\n",
        "image_files_list = []\n",
        "json_file = None\n",
        "for f in exported_files_list:\n",
        "  if f.endswith('.tfrecord.gz'):\n",
        "    image_files_list.append(f)\n",
        "  elif f.endswith('.json'):\n",
        "    json_file = f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNMIyqpXpZXo"
      },
      "outputs": [],
      "source": [
        "# Make sure the files are in the right order.\n",
        "image_files_list.sort()\n",
        "\n",
        "image_files_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dtkfuvbogcX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the contents of the mixer file to a JSON object.\n",
        "json_text = !gsutil cat {json_file}\n",
        "# Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "mixer = json.loads(json_text.nlstr)\n",
        "mixer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SfYnobAVn3DZ"
      },
      "outputs": [],
      "source": [
        "# Get relevant info from the JSON mixer file.\n",
        "patch_width = mixer['patchDimensions'][0]\n",
        "patch_height = mixer['patchDimensions'][1]\n",
        "patches = mixer['totalPatches']\n",
        "patch_dimensions_flat = [patch_width * patch_height, 1]\n",
        "\n",
        "# Note that the tensors are in the shape of a patch, one patch for each band.\n",
        "image_columns = [\n",
        "  tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32)\n",
        "    for k in BANDS\n",
        "]\n",
        "\n",
        "# Parsing dictionary.\n",
        "image_features_dict = dict(zip(BANDS, image_columns))\n",
        "\n",
        "# Note that you can make one dataset from many files by specifying a list.\n",
        "image_dataset = tf.data.TFRecordDataset(image_files_list, compression_type='GZIP')\n",
        "\n",
        "# Parsing function.\n",
        "def parse_image(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, image_features_dict)\n",
        "\n",
        "# Parse the data into tensors, one long tensor per patch.\n",
        "image_dataset = image_dataset.map(parse_image, num_parallel_calls=5)\n",
        "\n",
        "# Break our long tensors into many little ones.\n",
        "image_dataset = image_dataset.flat_map(\n",
        "  lambda features: tf.data.Dataset.from_tensor_slices(features)\n",
        ")\n",
        "\n",
        "# Turn the dictionary in each record into a tuple without a label.\n",
        "image_dataset = image_dataset.map(\n",
        "  lambda data_dict: (tf.transpose(list(data_dict.values())), )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szlYVG2bsn0m"
      },
      "outputs": [],
      "source": [
        "# Turn each patch into a batch.\n",
        "image_dataset = image_dataset.batch(patch_width * patch_height)\n",
        "\n",
        "iter(image_dataset).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4rAWJiC5oSdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ec60b6-9307-45ca-859c-8324d0adccb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1008/1008\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2279s\u001b[0m 2s/step\n"
          ]
        }
      ],
      "source": [
        "# Run prediction in batches, with as many steps as there are patches.\n",
        "predictions = model.predict(image_dataset, steps=patches, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjDB42OTqOEB"
      },
      "outputs": [],
      "source": [
        "# Note that the predictions come as a numpy array.  Check the first one.\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tr2xfuQpq04"
      },
      "outputs": [],
      "source": [
        "# Instantiate the writer.\n",
        "writer = tf.io.TFRecordWriter(OUTPUT_IMAGE_FILE)\n",
        "\n",
        "# Every patch-worth of predictions we'll dump an example into the output\n",
        "# file with a single feature that holds our predictions. Since our predictions\n",
        "# are already in the order of the exported data, the patches we create here\n",
        "# will also be in the right order.\n",
        "patch = [[]]\n",
        "cur_patch = 1\n",
        "for prediction in predictions:\n",
        "  patch[0].append(tf.argmax(prediction, 1))\n",
        "\n",
        "  # Once we've seen a patches-worth of class_ids...\n",
        "  if (len(patch[0]) == patch_width * patch_height):\n",
        "    print('Done with patch ' + str(cur_patch) + ' of ' + str(patches) + '...')\n",
        "    # Create an example\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'prediction': tf.train.Feature(\n",
        "              int64_list=tf.train.Int64List(\n",
        "                  value=patch[0]))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example to the file and clear our patch array so it's ready for\n",
        "    # another batch of class ids\n",
        "    writer.write(example.SerializeToString())\n",
        "    patch = [[]]\n",
        "    cur_patch += 1\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlZfsJtiqdYI"
      },
      "outputs": [],
      "source": [
        "!earthengine upload image --asset_id={OUTPUT_ASSET_ID} --pyramiding_policy=mode {OUTPUT_IMAGE_FILE} {json_file}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvrtiNs/Y36ALhngWOSWjx"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}